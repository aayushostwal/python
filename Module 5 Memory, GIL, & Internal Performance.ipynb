{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK7_Td3_T7ab"
   },
   "source": [
    "# Module 5: Memory, GIL, & Internal Performance\n",
    "\n",
    "### **The Scenario:**\n",
    "\n",
    "Youâ€™ve built a data processing pipeline that tracks 1 million delivery trucks in real-time. It worked great on your laptop with 10 trucks. But in production, your server is running out of RAM, and your CPU usage is stuck at 5% despite having 16 cores.\n",
    "\n",
    "\n",
    "### **The Goal:**\n",
    "By the end of this module, you will debug these specific crashes by looking inside the Python interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lef6LkIIT7ac"
   },
   "source": [
    "## Lesson 1: The hidden cost of \"Everything is an Object\"\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Your server logs show a `MemoryError`. You calculated that 1 million GPS points (just two integers, x and y) should take about **16MB**. But in reality, your Python script is eating **180MB**. Why?\n",
    "\n",
    "### The \"Aha!\" Moment\n",
    "\n",
    "Every custom object you create (like a `Truck` class) carries a hidden dictionary (`__dict__`) to store its attributes. This allows for dynamic attribute addition, but it costs significant memory.\n",
    "\n",
    "### The Solution: `__slots__`\n",
    "\n",
    "By defining `__slots__`, you tell Python: *\"Don't give me a dictionary. Just reserve space for these specific attributes.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bzZrNHRT7ac",
    "outputId": "7f045173-9bb4-4e78-b3d7-d41d0e9cc329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pympler in /Users/aayushostwal/miniconda3/envs/plat/lib/python3.9/site-packages (1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Bloated Object Real Size: 312 bytes\n",
      "Optimized Object Real Size: 96 bytes\n",
      "RAM Savings: 69.23%\n"
     ]
    }
   ],
   "source": [
    "!pip install pympler\n",
    "import sys\n",
    "from pympler import asizeof\n",
    "\n",
    "class BloatedTruck:\n",
    "    def __init__(self, lat, lng):\n",
    "        self.lat = lat\n",
    "        self.lng = lng\n",
    "\n",
    "class OptimizedTruck:\n",
    "    __slots__ = ['lat', 'lng']\n",
    "    def __init__(self, lat, lng):\n",
    "        self.lat = lat\n",
    "        self.lng = lng\n",
    "\n",
    "bloated = BloatedTruck(40.7128, -74.0060)\n",
    "lean = OptimizedTruck(40.7128, -74.0060)\n",
    "\n",
    "print(f\"Bloated Object Real Size: {asizeof.asizeof(bloated)} bytes\")\n",
    "print(f\"Optimized Object Real Size: {asizeof.asizeof(lean)} bytes\")\n",
    "print(f\"RAM Savings: {((asizeof.asizeof(bloated) - asizeof.asizeof(lean)) / asizeof.asizeof(bloated)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE764CjbT7ad"
   },
   "source": [
    "## Lesson 2: Sequence Memory (List vs. Tuple)\n",
    "\n",
    "### The Problem\n",
    "You noticed that even when you aren't adding new items, your lists seem to occupy more memory than expected.\n",
    "\n",
    "### The \"Aha!\" Moment\n",
    "Python lists use **Over-allocation**. To make `append()` fast ($O(1)$), Python grabs more memory than it needs so it doesn't have to resize every single time you add an item. Tuples, being immutable, are allocated exactly at the size needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfpKe3mYT7ad",
    "outputId": "c5b9532f-fe6b-4682-f98f-eb0f8ea5a0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watching List growth (Over-allocation in action):\n",
      "Items: 1 | Size in RAM: 88 bytes\n",
      "Items: 2 | Size in RAM: 88 bytes\n",
      "Items: 3 | Size in RAM: 88 bytes\n",
      "Items: 4 | Size in RAM: 88 bytes\n",
      "Items: 5 | Size in RAM: 120 bytes\n",
      "Items: 6 | Size in RAM: 120 bytes\n",
      "Items: 7 | Size in RAM: 120 bytes\n",
      "Items: 8 | Size in RAM: 120 bytes\n",
      "Items: 9 | Size in RAM: 184 bytes\n",
      "Items: 10 | Size in RAM: 184 bytes\n",
      "\n",
      "Static Tuple Size for 10 items: 120 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "lst = []\n",
    "print(\"Watching List growth (Over-allocation in action):\")\n",
    "for i in range(10):\n",
    "    lst.append(i)\n",
    "    print(f\"Items: {len(lst)} | Size in RAM: {sys.getsizeof(lst)} bytes\")\n",
    "\n",
    "tup = tuple(range(10))\n",
    "print(f\"\\nStatic Tuple Size for 10 items: {sys.getsizeof(tup)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JaPlHRoT7ad"
   },
   "source": [
    "## Lesson 3: Why 16 Cores aren't faster (The GIL)\n",
    "\n",
    "### The Problem\n",
    "You have a CPU-heavy calculation for each truck. You tried using `threading` to use all 16 cores, but the script is just as slow as the single-threaded version.\n",
    "\n",
    "### The \"Aha!\" Moment\n",
    "The **Global Interpreter Lock (GIL)** ensures only one thread executes Python bytecode at a time. This makes threads great for I/O (waiting for a network) but useless for heavy math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nip2wkLeT7ad",
    "outputId": "e720e524-db40-4644-c013-88233af2ecd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Time: 0.37s\n",
      "Threaded Time (GIL bottleneck): 0.33s\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def heavy_math(n):\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "\n",
    "COUNT = 10**7\n",
    "\n",
    "# Sequential execution\n",
    "start = time.time()\n",
    "heavy_math(COUNT*2)\n",
    "print(f\"Sequential Time: {time.time() - start:.2f}s\")\n",
    "\n",
    "# Threaded execution (Limited by GIL)\n",
    "threads = []\n",
    "for t in range(2):\n",
    "    threads.append(threading.Thread(target=heavy_math, args=(COUNT,)))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "[t.start() for t in threads]\n",
    "[t.join() for t in threads]\n",
    "\n",
    "print(f\"Threaded Time (GIL bottleneck): {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSMUkBQwT7ad"
   },
   "source": [
    "## Lesson 4: The Silent Leak (Circular References)\n",
    "\n",
    "### The Problem\n",
    "Your worker script runs for days, but the memory usage slowly creeps up until the OS kills it. You aren't storing data, so where is it going?\n",
    "\n",
    "### The \"Aha!\" Moment\n",
    "If Object A points to B, and B points to A, their reference counts never hit zero. Python's Garbage Collector (GC) has to find and destroy these \"islands.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXzOFdPyT7ad",
    "outputId": "d79ae099-6f5e-4f07-bea3-d5a7c0a11325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle created and references deleted.\n",
      "Unreachable objects before GC: 0\n",
      "GC manually triggered. Found and cleared 11 objects.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.partner = None\n",
    "    def __del__(self):\n",
    "        # In a real leak, this never prints until GC runs\n",
    "        pass\n",
    "\n",
    "def create_leak():\n",
    "    n1 = Node(\"A\")\n",
    "    n2 = Node(\"B\")\n",
    "    n1.partner = n2\n",
    "    n2.partner = n1\n",
    "    print(\"Cycle created and references deleted.\")\n",
    "\n",
    "gc.disable() # Let's simulate a busy system where GC hasn't triggered yet\n",
    "create_leak()\n",
    "\n",
    "print(f\"Unreachable objects before GC: {len(gc.garbage)}\")\n",
    "found = gc.collect()\n",
    "print(f\"GC manually triggered. Found and cleared {found} objects.\")\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25UsY813V30W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
