{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 5.a: Memory, GIL, & Internal Performance\n",
        "\n",
        "### The Scenario\n",
        "\n",
        "You've built a data processing pipeline that tracks 1 million delivery trucks in real-time. It worked great with 10 trucks on your laptop. But in production:\n",
        "- The server is running out of **RAM**\n",
        "- CPU usage is stuck at **5%** despite having 16 cores\n",
        "- Memory slowly creeps up until the OS **kills the process**\n",
        "\n",
        "### The Goal\n",
        "\n",
        "By the end of this module, you will:\n",
        "- Understand why objects use **more memory than expected**\n",
        "- Know when **threading helps** and when it doesn't (GIL)\n",
        "- Debug **memory leaks** from circular references\n",
        "- Optimize with `__slots__`, generators, and multiprocessing\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 1: The Hidden Cost of Objects\n",
        "\n",
        "### The Problem\n",
        "\n",
        "Your server logs show `MemoryError`. You calculated that 1 million GPS points (two floats: x, y) should take **16 MB**. But Python is eating **180 MB**. Why?\n",
        "\n",
        "### The \"Aha!\" Moment\n",
        "\n",
        "In Python, **everything is an object**. Every object has overhead:\n",
        "- Object header (~16 bytes)\n",
        "- Reference to `__dict__` (~8 bytes)\n",
        "- The `__dict__` itself (~104+ bytes)\n",
        "\n",
        "A simple class with two floats uses **300+ bytes** per instance!\n",
        "\n",
        "### The Solution: `__slots__`\n",
        "\n",
        "By defining `__slots__`, you tell Python: \"Don't give me a `__dict__`. Just reserve space for these specific attributes.\"\n",
        "\n",
        "| Class Type | Memory per Instance | Savings |\n",
        "|------------|---------------------|--------|\n",
        "| Regular class | ~488 bytes | - |\n",
        "| With `__slots__` | ~96 bytes | ~80% |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Regular class with __dict__\n",
        "class BloatedTruck:\n",
        "    def __init__(self, lat, lng):\n",
        "        self.lat = lat\n",
        "        self.lng = lng\n",
        "\n",
        "# Optimized class with __slots__\n",
        "class OptimizedTruck:\n",
        "    __slots__ = ['lat', 'lng']\n",
        "    def __init__(self, lat, lng):\n",
        "        self.lat = lat\n",
        "        self.lng = lng\n",
        "\n",
        "bloated = BloatedTruck(40.7128, -74.0060)\n",
        "optimized = OptimizedTruck(40.7128, -74.0060)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep size (includes __dict__ contents):\n",
            "  BloatedTruck:   488 bytes\n",
            "  OptimizedTruck: 96 bytes\n",
            "  Savings: 80.3%\n"
          ]
        }
      ],
      "source": [
        "# For accurate deep size, use pympler\n",
        "# ! pip install pympler\n",
        "try:\n",
        "    from pympler import asizeof\n",
        "    \n",
        "    print(\"Deep size (includes __dict__ contents):\")\n",
        "    print(f\"  BloatedTruck:   {asizeof.asizeof(bloated)} bytes\")\n",
        "    print(f\"  OptimizedTruck: {asizeof.asizeof(optimized)} bytes\")\n",
        "    \n",
        "    savings = (asizeof.asizeof(bloated) - asizeof.asizeof(optimized)) / asizeof.asizeof(bloated) * 100\n",
        "    print(f\"  Savings: {savings:.1f}%\")\n",
        "except ImportError:\n",
        "    print(\"Install pympler for deep size: pip install pympler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When to Use `__slots__`\n",
        "\n",
        "| Use `__slots__` | Avoid `__slots__` |\n",
        "|-----------------|-------------------|\n",
        "| Many instances (1000+) | Need dynamic attributes |\n",
        "| Memory-constrained | Multiple inheritance |\n",
        "| Fixed attributes | Rapid prototyping |\n",
        "| Performance-critical | Inheriting from non-slots class |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Lesson 2: List Over-Allocation\n",
        "\n",
        "### The Problem\n",
        "\n",
        "You noticed that even when you aren't adding new items, your lists use more memory than expected.\n",
        "\n",
        "### The \"Aha!\" Moment\n",
        "\n",
        "Python lists use **over-allocation** to make `append()` fast (amortized O(1)). When a list grows, Python allocates extra space so it doesn't resize on every append.\n",
        "\n",
        "### List vs Tuple Memory\n",
        "\n",
        "| Type | Allocation | Best For |\n",
        "|------|------------|----------|\n",
        "| `list` | Over-allocated | Dynamic collections |\n",
        "| `tuple` | Exact size | Fixed records |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Items    Size (bytes)         Total Size\n",
            "---------------------------------------------\n",
            "      1        88\t\t\t88 bytes\n",
            "      2        88\t\t\t88 bytes\n",
            "      3        88\t\t\t88 bytes\n",
            "      4        88\t\t\t88 bytes\n",
            "      5       120\t\t\t120 bytes\n",
            "      6       120\t\t\t120 bytes\n",
            "      7       120\t\t\t120 bytes\n",
            "      8       120\t\t\t120 bytes\n",
            "      9       184\t\t\t184 bytes\n",
            "     10       184\t\t\t184 bytes\n",
            "     11       184\t\t\t184 bytes\n",
            "     12       184\t\t\t184 bytes\n",
            "\n",
            "Tuple with 12 items: 136 bytes (exact size)\n"
          ]
        }
      ],
      "source": [
        "# Watch list over-allocation in action\n",
        "lst = []\n",
        "prev_size = 0\n",
        "\n",
        "print(f\"{'Items':8} {'Size (bytes)':20} {'Total Size'}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "for i in range(12):\n",
        "    lst.append(i)\n",
        "    size = sys.getsizeof(lst)\n",
        "    print(f\"{len(lst):7}{size:10}\\t\\t\\t{size} bytes\")\n",
        "\n",
        "# Compare with tuple\n",
        "tup = tuple(range(12))\n",
        "print(f\"\\nTuple with 12 items: {sys.getsizeof(tup)} bytes (exact size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Lesson 3: The GIL (Global Interpreter Lock)\n",
        "\n",
        "### The Problem\n",
        "\n",
        "You have CPU-heavy calculations for each truck. You tried `threading` to use all 16 cores, but the script is just as slow as the single-threaded version.\n",
        "\n",
        "### The \"Aha!\" Moment\n",
        "\n",
        "The **Global Interpreter Lock (GIL)** ensures only ONE thread executes Python bytecode at a time.\n",
        "\n",
        "### Threading vs Multiprocessing\n",
        "\n",
        "| Aspect | Threading | Multiprocessing |\n",
        "|--------|-----------|----------------|\n",
        "| GIL Impact | Blocked (one at a time) | Bypassed (separate GIL) |\n",
        "| Best For | I/O-bound (network, disk) | CPU-bound (math, processing) |\n",
        "| Memory | Shared | Isolated |\n",
        "| Overhead | Low | Higher (process spawn) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential: 0.24s\n",
            "Threaded:   0.23s (speedup: 1.05x)\n",
            "\n",
            "→ Threads don't help CPU-bound tasks due to GIL!\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "def cpu_heavy(n):\n",
        "    \"\"\"CPU-bound task: count down.\"\"\"\n",
        "    while n > 0:\n",
        "        n -= 1\n",
        "\n",
        "COUNT = 10_000_000\n",
        "\n",
        "# Sequential\n",
        "start = time.perf_counter()\n",
        "cpu_heavy(COUNT)\n",
        "cpu_heavy(COUNT)\n",
        "seq_time = time.perf_counter() - start\n",
        "print(f\"Sequential: {seq_time:.2f}s\")\n",
        "\n",
        "# Threaded (limited by GIL)\n",
        "threads = [\n",
        "    threading.Thread(target=cpu_heavy, args=(COUNT,)),\n",
        "    threading.Thread(target=cpu_heavy, args=(COUNT,)),\n",
        "]\n",
        "\n",
        "start = time.perf_counter()\n",
        "for t in threads:\n",
        "    t.start()\n",
        "for t in threads:\n",
        "    t.join()\n",
        "thread_time = time.perf_counter() - start\n",
        "\n",
        "print(f\"Threaded:   {thread_time:.2f}s (speedup: {seq_time/thread_time:.2f}x)\")\n",
        "print(\"\\n→ Threads don't help CPU-bound tasks due to GIL!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Solution: Multiprocessing\n",
        "\n",
        "Each process has its own Python interpreter and GIL, enabling true parallelism.\n",
        "\n",
        "**Python 3.13+ Note:** Experimental \"No-GIL\" builds are available with `python -X gil=0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multiprocessing:   0.18s (speedup: 1.35x)\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "\n",
        "multiprocessing.set_start_method('fork', force=True)\n",
        "# Each process gets its own GIL\n",
        "processes = [\n",
        "    multiprocessing.Process(target=cpu_heavy, args=(COUNT,)),\n",
        "    multiprocessing.Process(target=cpu_heavy, args=(COUNT,)),\n",
        "]\n",
        "\n",
        "start = time.perf_counter()\n",
        "for p in processes:\n",
        "    p.start()\n",
        "for p in processes:\n",
        "    p.join()\n",
        "thread_time = time.perf_counter() - start\n",
        "print(f\"Multiprocessing:   {thread_time:.2f}s (speedup: {seq_time/thread_time:.2f}x)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Lesson 4: Reference Counting & Garbage Collection\n",
        "\n",
        "### The Problem\n",
        "\n",
        "Your worker script runs for days, but memory slowly creeps up until the OS kills it. You aren't storing data, so where is it going?\n",
        "\n",
        "### The \"Aha!\" Moment\n",
        "\n",
        "Python uses **reference counting**: objects are freed when their reference count hits zero. But **circular references** (A → B → A) can never hit zero!\n",
        "\n",
        "### Memory Management\n",
        "\n",
        "| Mechanism | What It Does | When It Runs |\n",
        "|-----------|--------------|-------------|\n",
        "| Reference counting | Frees objects when refcount = 0 | Immediately |\n",
        "| Garbage collector | Finds circular references | Periodically |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial refcount: 2\n",
            "After alias: 3\n",
            "After container: 4\n",
            "After deletions: 2\n"
          ]
        }
      ],
      "source": [
        "# Reference counting\n",
        "data = [1, 2, 3]\n",
        "print(f\"Initial refcount: {sys.getrefcount(data)}\")\n",
        "\n",
        "alias = data  # +1 reference\n",
        "print(f\"After alias: {sys.getrefcount(data)}\")\n",
        "\n",
        "container = {'data': data}  # +1 reference\n",
        "print(f\"After container: {sys.getrefcount(data)}\")\n",
        "\n",
        "del alias  # -1 reference\n",
        "del container  # -1 reference\n",
        "print(f\"After deletions: {sys.getrefcount(data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Objects should be deleted, but...\n",
            "GC found and collected 8 objects\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "# Circular reference problem\n",
        "class Node:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.partner = None\n",
        "\n",
        "def create_cycle():\n",
        "    a = Node(\"A\")\n",
        "    b = Node(\"B\")\n",
        "    a.partner = b  # A → B\n",
        "    b.partner = a  # B → A (cycle!)\n",
        "    # When function returns, a and b go out of scope\n",
        "    # But refcounts never hit zero due to cycle!\n",
        "\n",
        "gc.disable()  # Disable GC to see the problem\n",
        "create_cycle()\n",
        "print(f\"Objects should be deleted, but...\")\n",
        "\n",
        "# Manually trigger GC\n",
        "collected = gc.collect()\n",
        "print(f\"GC found and collected {collected} objects\")\n",
        "\n",
        "gc.enable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Lesson 5: Memory Optimization Strategies\n",
        "\n",
        "### Quick Wins\n",
        "\n",
        "| Strategy | Savings | Use Case |\n",
        "|----------|---------|----------|\n",
        "| `__slots__` | ~70% | Classes with many instances |\n",
        "| Generators | Variable | Large sequences you iterate once |\n",
        "| `tuple` over `list` | ~20% | Fixed-size data |\n",
        "| `array.array` | ~50% | Homogeneous numeric data |\n",
        "| NumPy arrays | ~90% | Large numeric datasets |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List comprehension: 85,176 bytes\n",
            "Generator:          200 bytes\n",
            "\n",
            "→ Generator uses constant memory regardless of size!\n"
          ]
        }
      ],
      "source": [
        "# Generators vs Lists\n",
        "# List: creates all items in memory\n",
        "list_size = sys.getsizeof([x**2 for x in range(10000)])\n",
        "\n",
        "# Generator: creates items on-demand\n",
        "gen_size = sys.getsizeof(x**2 for x in range(10000))\n",
        "\n",
        "print(f\"List comprehension: {list_size:,} bytes\")\n",
        "print(f\"Generator:          {gen_size:,} bytes\")\n",
        "print(f\"\\n→ Generator uses constant memory regardless of size!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "### Object Memory\n",
        "\n",
        "| Issue | Cause | Solution |\n",
        "|-------|-------|----------|\n",
        "| Objects too large | `__dict__` overhead | Use `__slots__` |\n",
        "| Lists use extra memory | Over-allocation | Use tuple/array |\n",
        "| Memory grows slowly | Circular references | Use weakref, check GC |\n",
        "\n",
        "### Threading vs Multiprocessing\n",
        "\n",
        "| Task Type | Solution | Why |\n",
        "|-----------|----------|-----|\n",
        "| I/O-bound | Threading | GIL releases during I/O wait |\n",
        "| CPU-bound | Multiprocessing | Separate process = separate GIL |\n",
        "\n",
        "### Memory Optimization\n",
        "\n",
        "| Technique | When to Use |\n",
        "|-----------|------------|\n",
        "| `__slots__` | Many instances of a class |\n",
        "| Generators | Large sequences, iterate once |\n",
        "| `tuple` | Fixed-size immutable data |\n",
        "| NumPy | Large numeric computations |\n",
        "\n",
        "### Debugging Tools\n",
        "\n",
        "| Tool | Purpose |\n",
        "|------|--------|\n",
        "| `sys.getsizeof()` | Shallow object size |\n",
        "| `pympler.asizeof()` | Deep object size |\n",
        "| `gc.collect()` | Force garbage collection |\n",
        "| `tracemalloc` | Memory allocation tracking |\n",
        "\n",
        "---\n",
        "\n",
        "**Next in this notebook:** Module 5.b — How Python Compiler Works  \n",
        "\n",
        "**Then:** Module 6 — Advanced Python Concepts (Multiprocessing, AsyncIO, Context Managers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# Module 5.b: How Python Compiler Works\n",
        "\n",
        "### The Scenario\n",
        "\n",
        "You've heard that Python is \"interpreted,\" but your `.py` files seem to run fast after the first run. You see a `__pycache__` folder full of `.pyc` files. What is Python actually doing under the hood?\n",
        "\n",
        "### The Goal\n",
        "\n",
        "By the end of this section, you will:\n",
        "- Understand the **compilation pipeline**: source → bytecode\n",
        "- Know where **bytecode** (.pyc) is stored and when it's used\n",
        "- Use the **`dis`** module to inspect bytecode\n",
        "- See how this ties into **startup time** and **imports**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Compilation Pipeline\n",
        "\n",
        "Python does **not** compile to machine code like C or Rust. Instead, it compiles to **bytecode**—a low-level instruction set for the **CPython virtual machine (VM)**.\n",
        "\n",
        "### Stages\n",
        "\n",
        "| Stage | Input | Output | What Happens |\n",
        "|-------|--------|--------|----------------|\n",
        "| **Tokenize** | Source (`.py`) | Tokens | Lexical analysis: keywords, names, literals |\n",
        "| **Parse** | Tokens | AST (Abstract Syntax Tree) | Grammar rules → tree structure |\n",
        "| **Compile** | AST | Bytecode | Tree → stack-based instructions |\n",
        "| **Execute** | Bytecode | Result | VM interprets bytecode (or runs cached `.pyc`) |\n",
        "\n",
        "So when you run `python script.py`, Python **compiles** the source to bytecode (or loads cached bytecode), then the **interpreter** executes that bytecode. \"Interpreted\" means we interpret bytecode, not the raw `.py` text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bytecode and `.pyc` Files\n",
        "\n",
        "- **First run:** Python compiles `.py` → bytecode and writes it to `__pycache__/<module>.cpython-<version>.pyc`.\n",
        "- **Later runs:** If the `.pyc` is present and not older than the `.py` file, Python **loads the bytecode** and skips compilation → faster startup.\n",
        "- **Version-specific:** Bytecode is not portable across Python versions (e.g., 3.11 vs 3.12)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: How the Python Compiler Fits In\n",
        "\n",
        "| Concept | Takeaway |\n",
        "|--------|----------|\n",
        "| **Compilation** | `.py` is compiled to bytecode (tokens → AST → bytecode). |\n",
        "| **Caching** | Bytecode is cached in `__pycache__/*.pyc` to speed up imports and reruns. |\n",
        "| **Execution** | The CPython VM **interprets** bytecode; there is no built-in JIT in standard CPython. |\n",
        "| **Inspection** | Use `ast.parse()` / `compile()` and `dis.dis()` to see how your code becomes bytecode. |\n",
        "\n",
        "Understanding this pipeline helps explain **import cost** (compilation + bytecode load), **startup time**, and why tools like **Cython** (compile to C) or **PyPy** (JIT over bytecode) can change performance characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "plat3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
